<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">COMP7404 GroupB</h1>
            <p class="is-size-5">XUE Lin, HAN Yilong, QI Wenfei</p>
            <h2 class="title is-3 has-text-grey-dark">Boosting Semi-Supervised Scene Text Recognition via Viewing and Summarizing</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">Yadong Qu,</span>
                <span class="author-block">Yuxin Wang,</span>
                  <span class="author-block">Bangbang Zhou, Zixiao Wang, Hongtao Xie, Yongdong Zhang</span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Science and Technology of China, Hefei, China</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup> Advances in Neural Information Processing Systems 37 (NeurIPS 2024) </small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://papers.nips.cc/paper_files/paper/2024/file/be6122d1c7dec017c2bb1b168637f146-Paper-Conference.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/qqqyd/ViSu" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://papers.nips.cc/paper_files/paper/2024/hash/be6122d1c7dec017c2bb1b168637f146-Abstract-Conference.html" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span>Paper Link</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing scene text recognition (STR) methods struggle to recognize challenging texts, especially for artistic and severely distorted characters. The limitation lies in the insufficient exploration of character morphologies, including the monotonous
ness of widely used synthetic training data and the sensitivity of the model to
 character morphologies. To address these issues, inspired by the human learning
 process of viewing and summarizing, we facilitate the contrastive learning-based
 STR framework in a self-motivated manner by leveraging synthetic and real un
labeled data without any human cost. In the viewing process, to compensate for
 the simplicity of synthetic data and enrich character morphology diversity, we
 propose an Online Generation Strategy to generate background-free samples with
 diverse character styles. By excluding background noise distractions, the model
 is encouraged to focus on character morphology and generalize the ability to rec
ognize complex samples when trained with only simple synthetic data. To boost
 the summarizing process, we theoretically demonstrate the derivation error in the
 previous character contrastive loss, which mistakenly causes the sparsity in the
 intra-class distribution and exacerbates ambiguity on challenging samples. There
fore, a new Character Unidirectional Alignment Loss is proposed to correct this
 error and unify the representation of the same characters in all samples by align
ing the character features in the student model with the reference features in the
 teacher model. Extensive experiment results show that our method achieves SOTA
 performance (94.7% and 70.9% average accuracy on common benchmarks and
 Union14M-Benchmark). 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!--chatgpt静态图-->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-4">Results</h2>
      <div class="content">
        <p>(a)shows some images from synthetic datasets MJSynth and SynthText. (b) and (c) show several challenging test images. </p>
        <p> Due to severe distortion and artistic style, a character can visually differ from others of its category, and on the contrary, characters belonging to different
 categories can have similar appearances. </p>
        <img src="static/images/picture1.png" alt="First Image" class="image">
      </div>

      <div class="content">
        <p>(d) shows the distribution of challenging character features in ParSeq from t-SNE, where the characters cannot be clearly distinguished.</p>
        <p>(e) shows ViSu can clearly distinguish between images</p>
        <img src="static/images/picture2.png" alt="Second Image" class="image">
      </div>

      <div class="content">
        <p>The framework consists of the student and teacher model. Lrec, Lccr, Lcua mean recognition loss, character consistency regularization loss and character unidirectional alignment loss. Green
 and orange stand for labeled and unlabeled data, respectively</p>
        <img src="static/images/framwork.png" alt="Third Image" class="image">
      </div>

      <div class="content">
        <p>Comparison with SOTA methods on common benchmarks and Union-B.
          *means use publicly released checkpoints to evaluate the method. 
          + means reproduce the methods with the same configuration. 
          For training data: SL-MJSynth and SynthText; RL-Real labeled data; RU-Union14M-U; RU1-Book32, TextVQA, andST-VQA; RU2-Places2, OpenImages, and ImageNet
 ILSVRC2012. Cur, M-O, Art, Ctl, Sal, M-W, and Genrepresent Curve, Multi-Oriented, Artistic, Contextless, Salient, Multi-Words, and General. 
          P(M) means the model size.</p>
        <img src="static/images/table1.png" alt="Fourth Image" class="image">
      </div>
      
      <div class="content">
        <p>ComparisonwithSOTAmethodsonseveralchallengingbenchmarks.</p>
        <img src="static/images/table2.png" alt="Fifth Image" class="image">
      </div>
      
      <div class="content">
        <p>Ablation experiments with different training data.</p>
        <img src="static/images/table3.png" alt="Sixth Image" class="image">
      </div>

      <div class="content">
        <p>Ablation experiments with different configurations. URF means unified representation forms.
 OGS represents the online generation strategy. RU indicates whether to use real unlabeled data. CC1
 means only aligning with other characters in strong augmented images. CC2means the alignment
between strong augmented images and base images.</p>
        <img src="static/images/table4.png" alt="Seventh Image" class="image">
      </div>
    </div>
  </div>
</section>


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{NEURIPS2024_be6122d1,
 author = {Qu, Yadong and Wang, Yuxin and Zhou, Bangbang and Wang, Zixiao and Xie, Hongtao and Zhang, Yongdong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {105503--105527},
 publisher = {Curran Associates, Inc.},
 title = {Boosting Semi-Supervised Scene Text Recognition via Viewing and Summarizing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/be6122d1c7dec017c2bb1b168637f146-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License<br>For presentation of group work in COMP7404.</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
